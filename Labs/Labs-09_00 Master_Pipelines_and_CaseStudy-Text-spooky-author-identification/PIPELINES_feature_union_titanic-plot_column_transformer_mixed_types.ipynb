{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines and composite estimators\n",
    "Transformers are usually combined with classifiers, regressors or other estimators to build a composite estimator. The most common tool is a [Pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline). \n",
    "\n",
    "Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) serves multiple purposes here:\n",
    "\n",
    "* **Convenience and encapsulation**\n",
    "  * You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "\n",
    "* **Joint parameter selection**\n",
    "   * You can grid search over parameters of all estimators in the pipeline at once.\n",
    "* **Safety**\n",
    "  * Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n",
    "\n",
    "All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.).\n",
    "\n",
    "## Building pipelines\n",
    "The Pipeline is built using a list of (key, value) pairs, where the key is a string containing the name you want to give this step and value is an estimator object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n",
    "pipe = Pipeline(estimators)\n",
    "pipe \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utility function make_pipeline\n",
    "The utility function `make_pipeline` is a shorthand for constructing pipelines; it takes a variable number of estimators and returns a pipeline, filling in the names automatically:\n",
    "\n",
    "```python\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> from sklearn.naive_bayes import MultinomialNB\n",
    ">>> from sklearn.preprocessing import Binarizer\n",
    ">>> make_pipeline(Binarizer(), MultinomialNB()) \n",
    "Pipeline(memory=None,\n",
    "         steps=[('binarizer', Binarizer(copy=True, threshold=0.0)),\n",
    "                ('multinomialnb', MultinomialNB(alpha=1.0,\n",
    "                                                class_prior=None,\n",
    "                                                fit_prior=True))])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('binarizer', Binarizer(copy=True, threshold=0.0)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import Binarizer\n",
    "make_pipeline(Binarizer(), MultinomialNB()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipe.steps[0]\n",
    "The estimators of a pipeline are stored as a list in the steps attribute:\n",
    "    \n",
    "```python\n",
    ">>> pipe.steps[0]\n",
    "('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### access pipe step via step name\n",
    "and as a dict in named_steps:\n",
    "\n",
    "```python\n",
    ">>> pipe.named_steps['reduce_dim']\n",
    "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access parameters of each step via `<estimator>__<parameter>`\n",
    "Parameters of the estimators in the pipeline can be accessed using the `<estimator>__<parameter>` syntax:\n",
    "\n",
    "```python\n",
    ">>> pipe.set_params(clf__C=10) \n",
    "Pipeline(memory=None,\n",
    "         steps=[('reduce_dim', PCA(copy=True, iterated_power='auto',...)),\n",
    "                ('clf', SVC(C=10, cache_size=200, class_weight=None,...))])\n",
    "```\n",
    "\n",
    "Attributes of named_steps map to keys, enabling tab completion in interactive environments:\n",
    "\n",
    "```python\n",
    ">>> pipe.named_steps.reduce_dim is pipe.named_steps['reduce_dim']\n",
    "True\n",
    "```\n",
    "\n",
    "This is particularly important for doing grid searches:\n",
    "\n",
    "```python\n",
    ">>> from sklearn.model_selection import GridSearchCV\n",
    ">>> param_grid = dict(reduce_dim__n_components=[2, 5, 10],\n",
    "...                   clf__C=[0.1, 10, 100])\n",
    ">>> grid_search = GridSearchCV(pipe, param_grid=param_grid)\n",
    "```\n",
    "\n",
    "### Individual steps can be skipped or replaced as parameters\n",
    "Individual steps may also be replaced as parameters, and non-final steps may be ignored by setting them to `None`:\n",
    "\n",
    "```python\n",
    ">>> from sklearn.linear_model import LogisticRegression\n",
    ">>> param_grid = dict(reduce_dim=[None, PCA(5), PCA(10)],\n",
    "...                   clf=[SVC(), LogisticRegression()],\n",
    "...                   clf__C=[0.1, 10, 100])\n",
    ">>> grid_search = GridSearchCV(pipe, param_grid=param_grid)\n",
    "```\n",
    "\n",
    "Here reduce_dim can be\n",
    "* None \n",
    "* PCA(5)\n",
    "* PCA(10) \n",
    "\n",
    "In addition, individual steps may also be replaced as parameters. E.g.,\n",
    "\n",
    "clf can be\n",
    "* SVC()\n",
    "* or LogisticRegression()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching transformers: avoid repeated computation\n",
    "Fitting transformers may be computationally expensive. With its `memory parameter` set, Pipeline will cache each transformer after calling fit. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical. \n",
    "\n",
    "**A typical example is the case of a grid search in which the transformers can be fitted only once and reused for each configuration.**\n",
    "\n",
    "The parameter memory is needed in order to cache the transformers. memory can be either a string containing the directory where to cache the transformers or a `joblib.Memory object`:\n",
    "\n",
    "```python\n",
    ">>> from tempfile import mkdtemp\n",
    ">>> from shutil import rmtree\n",
    ">>> from sklearn.decomposition import PCA\n",
    ">>> from sklearn.svm import SVC\n",
    ">>> from sklearn.pipeline import Pipeline\n",
    ">>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n",
    ">>> cachedir = mkdtemp()\n",
    ">>> pipe = Pipeline(estimators, memory=cachedir)\n",
    ">>> pipe \n",
    "Pipeline(...,\n",
    "         steps=[('reduce_dim', PCA(copy=True,...)),\n",
    "                ('clf', SVC(C=1.0,...))])\n",
    ">>> # Clear the cache directory when you don't need it anymore\n",
    ">>> rmtree(cachedir)\n",
    "```\n",
    "\n",
    "**Warning: use with care!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* SKLearn documentation and examples [compose.html#pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline is often used in combination with FeatureUnion which concatenates the output of transformers into a composite feature space. TransformedTargetRegressor deals with transforming the target (i.e. log-transform y). In contrast, Pipelines only transform the observed data (X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeatureUnion: composite feature spaces\n",
    "\n",
    "FeatureUnion combines several transformer objects into a new transformer that combines their output. \n",
    "\n",
    "A FeatureUnion\n",
    "* A FeatureUnion takes a list of transformer objects. \n",
    "* During fitting, each of these is fit to the data independently. \n",
    "* The transformers are applied in parallel, and the feature matrices they output are concatenated side-by-side into a larger matrix.\n",
    "\n",
    "When you want to apply different transformations to each field of the data, see the related class `sklearn.compose.ColumnTransformer`.\n",
    "\n",
    "FeatureUnion serves the same purposes as Pipeline - convenience and joint parameter estimation and validation.\n",
    "\n",
    "FeatureUnion and Pipeline can be combined to create complex models.\n",
    "\n",
    "(A FeatureUnion has no way of checking whether two transformers might produce identical features. It only produces a union when the feature sets are disjoint, and making sure they are the caller’s responsibility.)\n",
    "\n",
    "A FeatureUnion is built using a list of (key, value) pairs, where the key is the name you want to give to a given transformation (an arbitrary string; it only serves as an identifier) and value is an estimator object:\n",
    "\n",
    "```python\n",
    ">>>\n",
    ">>> from sklearn.pipeline import FeatureUnion\n",
    ">>> from sklearn.decomposition import PCA\n",
    ">>> from sklearn.decomposition import KernelPCA\n",
    ">>> estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]\n",
    ">>> combined = FeatureUnion(estimators)\n",
    ">>> combined \n",
    "FeatureUnion(n_jobs=None,\n",
    "             transformer_list=[('linear_pca', PCA(copy=True,...)),\n",
    "                               ('kernel_pca', KernelPCA(alpha=1.0,...))],\n",
    "             transformer_weights=None)\n",
    "```\n",
    "\n",
    "Like pipelines, feature unions have a shorthand constructor called make_union that does not require explicit naming of the components.\n",
    "\n",
    "## Skip steps in the pipeline or replace\n",
    "Like Pipeline, individual steps may be replaced using set_params, and ignored by setting to 'drop':\n",
    "\n",
    "```python\n",
    ">>>\n",
    ">>> combined.set_params(kernel_pca='drop')\n",
    "... \n",
    "FeatureUnion(n_jobs=None,\n",
    "             transformer_list=[('linear_pca', PCA(copy=True,...)),\n",
    "                               ('kernel_pca', 'drop')],\n",
    "             transformer_weights=None)\n",
    "```\n",
    "\n",
    "Examples:\n",
    "\n",
    "* [Concatenating multiple feature extraction methods](https://scikit-learn.org/stable/auto_examples/compose/plot_feature_union.html#sphx-glr-auto-examples-compose-plot-feature-union-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColumnTransformer for heterogeneous data\n",
    "\n",
    "Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using pandas. Processing your data before passing it to scikit-learn might be problematic for one of the following reasons:\n",
    "\n",
    "Incorporating statistics from test data into the preprocessors makes cross-validation scores unreliable (known as data leakage), for example in the case of scalers or imputing missing values.\n",
    "You may want to include the parameters of the preprocessors in a parameter search.\n",
    "The ColumnTransformer helps performing different transformations for different columns of the data, within a Pipeline that is safe from data leakage and that can be parametrized. [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer) works on arrays, sparse matrices, and pandas DataFrames.\n",
    "\n",
    "For more background, see [here](https://scikit-learn.org/stable/modules/compose.html#featureunion-composite-feature-spaces).\n",
    "\n",
    "To each column, a different transformation can be applied, such as preprocessing or a specific feature extraction method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import pandas as pd\n",
    ">>> X = pd.DataFrame(\n",
    "...     {'city': ['London', 'London', 'Paris', 'Sallisaw'],\n",
    "...      'title': [\"His Last Bow\", \"How Watson Learned the Trick\",\n",
    "...                \"A Moveable Feast\", \"The Grapes of Wrath\"],\n",
    "...      'expert_rating': [5, 3, 4, 5],\n",
    "...      'user_rating': [4, 5, 4, 3]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>title</th>\n",
       "      <th>expert_rating</th>\n",
       "      <th>user_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>London</td>\n",
       "      <td>His Last Bow</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>London</td>\n",
       "      <td>How Watson Learned the Trick</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paris</td>\n",
       "      <td>A Moveable Feast</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sallisaw</td>\n",
       "      <td>The Grapes of Wrath</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       city                         title  expert_rating  user_rating\n",
       "0    London                  His Last Bow              5            4\n",
       "1    London  How Watson Learned the Trick              3            5\n",
       "2     Paris              A Moveable Feast              4            4\n",
       "3  Sallisaw           The Grapes of Wrath              5            3"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data, we might want to encode the 'city' column as a categorical variable using preprocessing.OneHotEncoder but apply a feature_extraction.text.CountVectorizer to the 'title' column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say `city_category` and `title_bow`. By default, the remaining rating columns are ignored (remainder='drop'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_trans.fit(X) :ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('city_category', OneHotEncoder(categorical_features=None, categories=None, dtype='int',\n",
      "       handle_unknown='error', n_values=None, sparse=True), ['city']), ('title_bow', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encod...accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'title')])\n",
      "\n",
      "column_trans.get_feature_names() :['city_category__x0_London', 'city_category__x0_Paris', 'city_category__x0_Sallisaw', 'title_bow__bow', 'title_bow__feast', 'title_bow__grapes', 'title_bow__his', 'title_bow__how', 'title_bow__last', 'title_bow__learned', 'title_bow__moveable', 'title_bow__of', 'title_bow__the', 'title_bow__trick', 'title_bow__watson', 'title_bow__wrath']\n",
      "\n",
      "column_trans.transform(X).toarray() :[[1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "column_trans = ColumnTransformer(\n",
    "    [('city_category', OneHotEncoder(dtype='int'),['city']),\n",
    "     ('title_bow', CountVectorizer(), 'title')],\n",
    "    remainder='drop')\n",
    "\n",
    "print(f\"column_trans.fit(X) :{column_trans.fit(X)}\\n\")\n",
    "print(f\"column_trans.get_feature_names() :{list(column_trans.get_feature_names())}\\n\")\n",
    "print(f\"column_trans.transform(X).toarray() :{column_trans.transform(X).toarray()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, \n",
    "\n",
    "* the` CountVectorizer expects` a 1D array as input and therefore the columns were specified as a string ('title').\n",
    "* However, `preprocessing.OneHotEncoder` as most of other transformers expects 2D data, therefore in that case you need to specify the column as a list of strings `(['city'])`.\n",
    "\n",
    "## column selection depends on the input type\n",
    "Apart from a scalar or a single item list, the column selection can be specified as a list of multiple items, an integer array, a slice, or a boolean mask. \n",
    "\n",
    "* Strings can reference columns if the input is a DataFrame, \n",
    "* integers are always interpreted as the positional columns.\n",
    "\n",
    "## The `remainder` parameter\n",
    "We can keep the remaining rating columns by setting remainder='passthrough'. The values are appended to the end of the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 4],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 5],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 4],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 5, 3]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_trans = ColumnTransformer(\n",
    "    [('city_category', OneHotEncoder(dtype='int'),['city']),\n",
    "     ('title_bow', CountVectorizer(), 'title')],\n",
    "    remainder='passthrough')\n",
    "\n",
    "column_trans.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `remainder` parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1. , 0.5],\n",
       "       [0. , 1. ],\n",
       "       [0.5, 0.5],\n",
       "       [1. , 0. ]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "column_trans = ColumnTransformer(\n",
    "    [('city_category', OneHotEncoder(), ['city']),\n",
    "     ('title_bow', CountVectorizer(), 'title')],\n",
    "    remainder=MinMaxScaler())\n",
    "\n",
    "column_trans.fit_transform(X)[:, -2:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_column_transformer\n",
    "The `make_column_transformer` function is available to more easily create a ColumnTransformer object. Specifically, the names will be given automatically. The equivalent for the above example would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=None,\n",
       "         remainder=MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       "         sparse_threshold=0.3, transformer_weights=None,\n",
       "         transformers=[('onehotencoder', OneHotEncoder(categorical_features=None, categories=None,\n",
       "       dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "       n_values=None, sparse=True), ['city']), ('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dty...accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None), 'title')])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "column_trans = make_column_transformer(\n",
    "    (OneHotEncoder(), ['city']),\n",
    "    (CountVectorizer(), 'title'),\n",
    "    remainder=MinMaxScaler())\n",
    "column_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Titanic Casestudy: Column Transformer with Mixed Types\n",
    "\n",
    "\n",
    "This example illustrates how to apply different preprocessing and\n",
    "feature extraction pipelines to different subsets of features,\n",
    "using :class:`sklearn.compose.ColumnTransformer`.\n",
    "This is particularly handy for the case of datasets that contain\n",
    "heterogeneous data types, since we may want to scale the\n",
    "numeric features and one-hot encode the categorical ones.\n",
    "\n",
    "In this example, the numeric data is standard-scaled after\n",
    "mean-imputation, while the categorical data is one-hot\n",
    "encoded after imputing missing values with a new category\n",
    "(``'missing'``).\n",
    "\n",
    "Finally, the preprocessing pipeline is integrated in a\n",
    "full prediction pipeline using :class:`sklearn.pipeline.Pipeline`,\n",
    "together with a simple classification model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train           shape: (755, 13)\n",
      "X validation      shape: (158, 13)\n",
      "X test            shape: (134, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>1</td>\n",
       "      <td>Newell, Miss. Madeleine</td>\n",
       "      <td>female</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35273</td>\n",
       "      <td>113.2750</td>\n",
       "      <td>D36</td>\n",
       "      <td>C</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lexington, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>3</td>\n",
       "      <td>Davies, Mr. John Samuel</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>A/4 48871</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>West Bromwich, England Pontiac, MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>3</td>\n",
       "      <td>Karaic, Mr. Milan</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349246</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>3</td>\n",
       "      <td>Moor, Master. Meier</td>\n",
       "      <td>male</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>392096</td>\n",
       "      <td>12.4750</td>\n",
       "      <td>E121</td>\n",
       "      <td>S</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1</td>\n",
       "      <td>Ismay, Mr. Joseph Bruce</td>\n",
       "      <td>male</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112058</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>B52 B54 B56</td>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Liverpool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pclass                     name     sex   age  sibsp  parch     ticket  \\\n",
       "213        1  Newell, Miss. Madeleine  female  31.0      1      0      35273   \n",
       "754        3  Davies, Mr. John Samuel    male  21.0      2      0  A/4 48871   \n",
       "912        3        Karaic, Mr. Milan    male  30.0      0      0     349246   \n",
       "1025       3      Moor, Master. Meier    male   6.0      0      1     392096   \n",
       "170        1  Ismay, Mr. Joseph Bruce    male  49.0      0      0     112058   \n",
       "\n",
       "          fare        cabin embarked boat  body  \\\n",
       "213   113.2750          D36        C    6   NaN   \n",
       "754    24.1500          NaN        S  NaN   NaN   \n",
       "912     7.8958          NaN        S  NaN   NaN   \n",
       "1025   12.4750         E121        S   14   NaN   \n",
       "170     0.0000  B52 B54 B56        S    C   NaN   \n",
       "\n",
       "                               home.dest  \n",
       "213                        Lexington, MA  \n",
       "754   West Bromwich, England Pontiac, MI  \n",
       "912                                  NaN  \n",
       "1025                                 NaN  \n",
       "170                            Liverpool  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data from Titanic dataset.\n",
    "titanic_url = ('https://raw.githubusercontent.com/amueller/'\n",
    "               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')\n",
    "data = pd.read_csv(titanic_url)\n",
    "\n",
    "\n",
    "# Split the provided training data into training and validationa and test\n",
    "# The kaggle evaluation test set has no labels\n",
    "#\n",
    "X = data.drop('survived', axis=1)\n",
    "y = data['survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "#X_kaggle_test= datasets[\"test\"][features]\n",
    "# y_test = datasets[\"application_test\"]['TARGET']   #why no  TARGET?!! (hint: kaggle competition)\n",
    "print(f\"X train           shape: {X_train.shape}\")\n",
    "print(f\"X validation      shape: {X_valid.shape}\")\n",
    "print(f\"X test            shape: {X_test.shape}\")\n",
    "#print(f\"X X_kaggle_test   shape: {X_kaggle_test.shape}\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.769\n"
     ]
    }
   ],
   "source": [
    "# We will train our classifier with the following features:\n",
    "# Numeric Features:\n",
    "# - age: float.\n",
    "# - fare: float.\n",
    "# Categorical Features:\n",
    "# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n",
    "# - sex: categories encoded as strings {'female', 'male'}.\n",
    "# - pclass: ordinal integers {1, 2, 3}.\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_features = ['age', 'fare', 'parch', 'sibsp',]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs'))])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Valid Acc</th>\n",
       "      <th>Test  Acc</th>\n",
       "      <th>Train LogLoss</th>\n",
       "      <th>Valid LogLoss</th>\n",
       "      <th>Test  LogLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.7947</td>\n",
       "      <td>0.7911</td>\n",
       "      <td>0.7687</td>\n",
       "      <td>0.4647</td>\n",
       "      <td>0.4344</td>\n",
       "      <td>0.4783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   exp_name  Train Acc  Valid Acc  Test  Acc  Train LogLoss  Valid LogLoss  \\\n",
       "0  baseline     0.7947     0.7911     0.7687         0.4647         0.4344   \n",
       "\n",
       "   Test  LogLoss  \n",
       "0         0.4783  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "del expLog\n",
    "\n",
    "exp_name = \"baseline\"\n",
    "try:\n",
    "    expLog\n",
    "except NameError:\n",
    "   expLog = pd.DataFrame(columns=[\"exp_name\", \n",
    "                                   \"Train Acc\", \n",
    "                                   \"Valid Acc\",\n",
    "                                   \"Test  Acc\",\n",
    "                                   \"Train LogLoss\", \n",
    "                                   \"Valid LogLoss\",\n",
    "                                   \"Test  LogLoss\"\n",
    "                                  ])\n",
    "\n",
    "expLog.loc[len(expLog)] = [f\"{exp_name}\"] + list(np.round(\n",
    "               [accuracy_score(y_train, model.predict(X_train)), \n",
    "                accuracy_score(y_valid, model.predict(X_valid)),\n",
    "                accuracy_score(y_test, model.predict(X_test)),\n",
    "                log_loss(y_train, model.predict_proba(X_train)),\n",
    "                log_loss(y_valid, model.predict_proba(X_valid)),\n",
    "                log_loss(y_test, model.predict_proba(X_test))],\n",
    "    4)) \n",
    "expLog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Log loss Evaluation metrics for 3-class problem\n",
    "\n",
    "Here we explain log loss for three examples. Submissions are evaluated using multi-class logarithmic loss. Each id has one true class. For each id, you must submit a predicted probability for each author. The formula is then:\n",
    "\n",
    "\n",
    "$$log loss = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^My_{ij}\\log(p_{ij}),$$\n",
    "\n",
    "where N is the number of observations in the test set, M is the number of class labels (3 classes), log is the natural logarithm, yij is 1 if observation i belongs to class j and 0 otherwise, and pij is the predicted probability that observation i belongs to class j.  A log loss of zero is best but is rarely achieved.\n",
    "\n",
    "The submitted probabilities for a given sentences are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with max(min(p,1−10−15),10−15).\n",
    "\n",
    "Let's try to interpret the logloss for a random model (i.e., predict a probability of $\\frac{1}{3}$ for each of the three classes for all test cases):\n",
    "\n",
    "```python\n",
    "\n",
    "print(f\"baseline log loss is {-np.log(1/3):0.3f}\")   #baseline log loss is 1.0986122886681098 \n",
    "print(f\"baseline class prob  {np.exp(-1.0986122886681098):0.3f}\")   #baseline log loss is 1.0986122886681098 \n",
    "print(f\"baseline log loss is {-np.log(1):0.3f}\")   #predicted probability of true class is 1, then zero loss! \n",
    "\n",
    "baseline log loss is 1.099\n",
    "baseline class prob  0.333\n",
    "baseline log loss is -0.000\n",
    "```\n",
    "\n",
    "We only look at the predicted probability of the true class. It should be 1 but it is $\\frac{1}{3}$.\n",
    "\n",
    "### logloss discussion\n",
    "\n",
    "The logloss for the baseline submission is $0.83$. That means we are predicting a probability of 0.43 for the target class; it should be 1 or close to 1. So we have lots of room for improvement!!\n",
    "\n",
    "```python\n",
    "\n",
    "np.exp(-0.83) # means we are predicting a probability of 0.43 for the target class; it should be 1 or close to 1.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline log loss is 1.099\n",
      "baseline class prob  0.333\n",
      "baseline log loss is -0.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"baseline log loss is {-np.log(1/3):0.3f}\")   #baseline log loss is 1.0986122886681098 \n",
    "print(f\"baseline class prob  {np.exp(-1.0986122886681098):0.3f}\")   #baseline log loss is 1.0986122886681098 \n",
    "print(f\"baseline log loss is {-np.log(1):0.3f}\")   #predicted probability of true class is 1, then zero loss! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4360492863215356"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-0.83) # means we are predicting a probability of 0.43 for the target class; it should be 1 or close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  prediction pipeline in a grid search\n",
    "\n",
    "Grid search can also be performed on the different preprocessing steps\n",
    " defined in the ``ColumnTransformer`` object, together with the classifier's\n",
    " hyperparameters as part of the ``Pipeline``.\n",
    " We will search for both the imputer strategy of the numeric preprocessing\n",
    " and the regularization parameter of the logistic regression using\n",
    " :class:`sklearn.model_selection.GridSearchCV`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search with multi-level pipelines\n",
    "Grid search is all about figuring out what the best hyperparameters of the data set is. To see the list of all the possible things you could fine tune, call get_params().keys() on your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['memory',\n",
       " 'steps',\n",
       " 'preprocessor',\n",
       " 'classifier',\n",
       " 'preprocessor__n_jobs',\n",
       " 'preprocessor__remainder',\n",
       " 'preprocessor__sparse_threshold',\n",
       " 'preprocessor__transformer_weights',\n",
       " 'preprocessor__transformers',\n",
       " 'preprocessor__num',\n",
       " 'preprocessor__cat',\n",
       " 'preprocessor__num__memory',\n",
       " 'preprocessor__num__steps',\n",
       " 'preprocessor__num__imputer',\n",
       " 'preprocessor__num__scaler',\n",
       " 'preprocessor__num__imputer__copy',\n",
       " 'preprocessor__num__imputer__fill_value',\n",
       " 'preprocessor__num__imputer__missing_values',\n",
       " 'preprocessor__num__imputer__strategy',\n",
       " 'preprocessor__num__imputer__verbose',\n",
       " 'preprocessor__num__scaler__copy',\n",
       " 'preprocessor__num__scaler__with_mean',\n",
       " 'preprocessor__num__scaler__with_std',\n",
       " 'preprocessor__cat__memory',\n",
       " 'preprocessor__cat__steps',\n",
       " 'preprocessor__cat__imputer',\n",
       " 'preprocessor__cat__onehot',\n",
       " 'preprocessor__cat__imputer__copy',\n",
       " 'preprocessor__cat__imputer__fill_value',\n",
       " 'preprocessor__cat__imputer__missing_values',\n",
       " 'preprocessor__cat__imputer__strategy',\n",
       " 'preprocessor__cat__imputer__verbose',\n",
       " 'preprocessor__cat__onehot__categorical_features',\n",
       " 'preprocessor__cat__onehot__categories',\n",
       " 'preprocessor__cat__onehot__dtype',\n",
       " 'preprocessor__cat__onehot__handle_unknown',\n",
       " 'preprocessor__cat__onehot__n_values',\n",
       " 'preprocessor__cat__onehot__sparse',\n",
       " 'classifier__C',\n",
       " 'classifier__class_weight',\n",
       " 'classifier__dual',\n",
       " 'classifier__fit_intercept',\n",
       " 'classifier__intercept_scaling',\n",
       " 'classifier__max_iter',\n",
       " 'classifier__multi_class',\n",
       " 'classifier__n_jobs',\n",
       " 'classifier__penalty',\n",
       " 'classifier__random_state',\n",
       " 'classifier__solver',\n",
       " 'classifier__tol',\n",
       " 'classifier__verbose',\n",
       " 'classifier__warm_start']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best logistic regression from grid search: 0.769\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=10, iid=False)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print((\"best logistic regression from grid search: %.3f\"\n",
    "       % grid_search.score(X_test, y_test)))\n",
    "model = grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Valid Acc</th>\n",
       "      <th>Test  Acc</th>\n",
       "      <th>Train LogLoss</th>\n",
       "      <th>Valid LogLoss</th>\n",
       "      <th>Test  LogLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.7947</td>\n",
       "      <td>0.7911</td>\n",
       "      <td>0.7687</td>\n",
       "      <td>0.4647</td>\n",
       "      <td>0.4344</td>\n",
       "      <td>0.4783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline_gridsearch</td>\n",
       "      <td>0.7960</td>\n",
       "      <td>0.7911</td>\n",
       "      <td>0.7687</td>\n",
       "      <td>0.4709</td>\n",
       "      <td>0.4381</td>\n",
       "      <td>0.4774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              exp_name  Train Acc  Valid Acc  Test  Acc  Train LogLoss  \\\n",
       "0             baseline     0.7947     0.7911     0.7687         0.4647   \n",
       "1  baseline_gridsearch     0.7960     0.7911     0.7687         0.4709   \n",
       "\n",
       "   Valid LogLoss  Test  LogLoss  \n",
       "0         0.4344         0.4783  \n",
       "1         0.4381         0.4774  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "exp_name = \"baseline_gridsearch\"\n",
    "try:\n",
    "    expLog\n",
    "except NameError:\n",
    "   expLog = pd.DataFrame(columns=[\"exp_name\", \n",
    "                                   \"Train Acc\", \n",
    "                                   \"Valid Acc\",\n",
    "                                   \"Test  Acc\",\n",
    "                                   \"Train LogLoss\", \n",
    "                                   \"Valid LogLoss\",\n",
    "                                   \"Test  LogLoss\"\n",
    "                                  ])\n",
    "\n",
    "expLog.loc[len(expLog)] = [f\"{exp_name}\"] + list(np.round(\n",
    "               [accuracy_score(y_train, model.predict(X_train)), \n",
    "                accuracy_score(y_valid, model.predict(X_valid)),\n",
    "                accuracy_score(y_test, model.predict(X_test)),\n",
    "                log_loss(y_train, model.predict_proba(X_train)),\n",
    "                log_loss(y_valid, model.predict_proba(X_valid)),\n",
    "                log_loss(y_test, model.predict_proba(X_test))],\n",
    "    4)) \n",
    "expLog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title-based transformer from Name feature\n",
    "\n",
    "### Write the title transformer class and debug\n",
    "Let's write the title transformer class step by step and unit test it.\n",
    "\n",
    "* build a TitleAdder transform\n",
    "* Test the TitleAdder transform using a simple one-step pipeline\n",
    "* Test the TitleAdder transform using a two-step pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>1</td>\n",
       "      <td>Newell, Miss. Madeleine</td>\n",
       "      <td>female</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35273</td>\n",
       "      <td>113.2750</td>\n",
       "      <td>D36</td>\n",
       "      <td>C</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lexington, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>3</td>\n",
       "      <td>Davies, Mr. John Samuel</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>A/4 48871</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>West Bromwich, England Pontiac, MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>3</td>\n",
       "      <td>Karaic, Mr. Milan</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349246</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>3</td>\n",
       "      <td>Moor, Master. Meier</td>\n",
       "      <td>male</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>392096</td>\n",
       "      <td>12.4750</td>\n",
       "      <td>E121</td>\n",
       "      <td>S</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1</td>\n",
       "      <td>Ismay, Mr. Joseph Bruce</td>\n",
       "      <td>male</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112058</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>B52 B54 B56</td>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Liverpool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pclass                     name     sex   age  sibsp  parch     ticket  \\\n",
       "213        1  Newell, Miss. Madeleine  female  31.0      1      0      35273   \n",
       "754        3  Davies, Mr. John Samuel    male  21.0      2      0  A/4 48871   \n",
       "912        3        Karaic, Mr. Milan    male  30.0      0      0     349246   \n",
       "1025       3      Moor, Master. Meier    male   6.0      0      1     392096   \n",
       "170        1  Ismay, Mr. Joseph Bruce    male  49.0      0      0     112058   \n",
       "\n",
       "          fare        cabin embarked boat  body  \\\n",
       "213   113.2750          D36        C    6   NaN   \n",
       "754    24.1500          NaN        S  NaN   NaN   \n",
       "912     7.8958          NaN        S  NaN   NaN   \n",
       "1025   12.4750         E121        S   14   NaN   \n",
       "170     0.0000  B52 B54 B56        S    C   NaN   \n",
       "\n",
       "                               home.dest  \n",
       "213                        Lexington, MA  \n",
       "754   West Bromwich, England Pontiac, MI  \n",
       "912                                  NaN  \n",
       "1025                                 NaN  \n",
       "170                            Liverpool  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()  #just remind ourselves of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "\n",
    "class TitleAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features=None): # no *args or **kargs\n",
    "        self.features = features\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        df = pd.DataFrame(X, columns=self.features)\n",
    "        df['Title'] = df['name'].apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\n",
    "        # Apply the necessary transformations to obtain the 5 title categories\n",
    "        # (Mr, Mrs, Miss, Master, Other) like it was done in section 5.1.2\n",
    "        df['Title'] = df['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'})\n",
    "        df['Title'] = df['Title'].replace(['Don', 'Dona', 'Rev', 'Dr','Major', 'Lady', 'Sir', \n",
    "                                           'Col', 'Capt', 'Countess', 'Jonkheer'],'Other')\n",
    "        #drop text features as we need to switch from a generic dateframe to a Numpy Array with the title column\n",
    "        df.drop('name', axis=1, inplace=True)\n",
    "        return np.array(df.values)  #return a Numpy Array to observe the pipeline protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (755, 13)\n",
      "\n",
      "X_train['name'][0:5]: \n",
      "213     Newell, Miss. Madeleine\n",
      "754     Davies, Mr. John Samuel\n",
      "912           Karaic, Mr. Milan\n",
      "1025        Moor, Master. Meier\n",
      "170     Ismay, Mr. Joseph Bruce\n",
      "Name: name, dtype: object\n",
      "Test driver: \n",
      "[['Miss']\n",
      " ['Mr']\n",
      " ['Mr']\n",
      " ['Master']\n",
      " ['Mr']]\n"
     ]
    }
   ],
   "source": [
    "def test_driver_title_simple_one_step_pipeline():\n",
    "    print(f\"X_train.shape: {X_train.shape}\\n\")\n",
    "    print(f\"X_train['name'][0:5]: \\n{X_train['name'][0:5]}\")\n",
    "    test_pipeline = make_pipeline(TitleAdder(['name']))\n",
    "    return(test_pipeline.fit_transform(X_train))\n",
    "print(f\"Test driver: \\n{test_driver_title_simple_one_step_pipeline()[0:5,:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (755, 13)\n",
      "\n",
      "X_train['name'][0:5]: \n",
      "213     Newell, Miss. Madeleine\n",
      "754     Davies, Mr. John Samuel\n",
      "912           Karaic, Mr. Milan\n",
      "1025        Moor, Master. Meier\n",
      "170     Ismay, Mr. Joseph Bruce\n",
      "Name: name, dtype: object\n",
      "Test driver: \n",
      "  (0, 1)\t1.0\n",
      "  (1, 2)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 2)\t1.0\n",
      "\n",
      " OHE for title has 5 unique values:  ['Master' 'Miss' 'Mr' 'Mrs' 'Other']\n",
      "\n",
      "OHE for first 5 examples \n",
      "Miss, Mr, Mr, Master, Mr\n",
      "  (0, 1)\t1.0\n",
      "  (1, 2)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 2)\t1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Master', 'Miss', 'Mr', 'Mrs', 'Other'], dtype=object)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_driver_title_simple_TWO_step_pipeline():\n",
    "    test_pipeline = make_pipeline(TitleAdder(['name']), OneHotEncoder(handle_unknown='ignore'))\n",
    "    return(test_pipeline.fit_transform(X_train))\n",
    "\n",
    "test = test_driver_for_title()\n",
    "print(f\"Test driver: \\n{test_driver_title_simple_TWO_step_pipeline()[0:5,:]}\")\n",
    "print(f\"\\n OHE for title has 5 unique values:  {np.unique(test)}\\n\")\n",
    "print(f\"OHE for first 5 examples \\nMiss, Mr, Mr, Master, Mr\\n{test_driver_title_simple_TWO_step_pipeline()[0:5,:]}\")\n",
    "test.shape\n",
    "np.unique(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical pipeline: not inline\n",
    "\n",
    "Here we define subpiplines first and then incorporate them into the main pipeline. In the next section  all subpipelines are **inlined**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.791\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data from Titanic dataset.\n",
    "titanic_url = ('https://raw.githubusercontent.com/amueller/'\n",
    "               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')\n",
    "data = pd.read_csv(titanic_url)\n",
    "\n",
    "# We will train our classifier with the following features:\n",
    "# Numeric Features:\n",
    "# - age: float.\n",
    "# - fare: float.\n",
    "# Categorical Features:\n",
    "# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n",
    "# - sex: categories encoded as strings {'female', 'male'}.\n",
    "# - pclass: ordinal integers {1, 2, 3}.\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "#\n",
    "numeric_features = ['age', 'fare', 'parch', 'sibsp',]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Title transform\n",
    "title_features = ['name']\n",
    "title_transformer = Pipeline(steps=[\n",
    "    ('title_prep', TitleAdder()),\n",
    "    ('title', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('title', title_transformer, title_features),\n",
    "    ])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs'))])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical pipeline inline\n",
    "\n",
    "Previously we defined pipelines first and then incorporated them into the main pipeline. Here inline all subpipelines are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.791\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data from Titanic dataset.\n",
    "titanic_url = ('https://raw.githubusercontent.com/amueller/'\n",
    "               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')\n",
    "data = pd.read_csv(titanic_url)\n",
    "\n",
    "# We will train our classifier with the following features:\n",
    "# Numeric Features:\n",
    "# - age: float.\n",
    "# - fare: float.\n",
    "# Categorical Features:\n",
    "# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n",
    "# - sex: categories encoded as strings {'female', 'male'}.\n",
    "# - pclass: ordinal integers {1, 2, 3}.\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "#\n",
    "\n",
    "# Features for the different transformers\n",
    "numeric_features = ['age', 'fare', 'parch', 'sibsp',]\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "title_features = ['name']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num',  \n",
    "         Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())  ]), \n",
    "         numeric_features),\n",
    "\n",
    "        ('cat',  Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))  ] ),\n",
    "         categorical_features),\n",
    "\n",
    "        ('title',  Pipeline(steps=[\n",
    "            ('title_prep', TitleAdder()),\n",
    "            ('title', OneHotEncoder(handle_unknown='ignore')) ] ),\n",
    "        title_features),\n",
    "    ])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs'))])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Valid Acc</th>\n",
       "      <th>Test  Acc</th>\n",
       "      <th>Train LogLoss</th>\n",
       "      <th>Valid LogLoss</th>\n",
       "      <th>Test  LogLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.7947</td>\n",
       "      <td>0.7911</td>\n",
       "      <td>0.7687</td>\n",
       "      <td>0.4647</td>\n",
       "      <td>0.4344</td>\n",
       "      <td>0.4783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline_gridsearch</td>\n",
       "      <td>0.7960</td>\n",
       "      <td>0.7911</td>\n",
       "      <td>0.7687</td>\n",
       "      <td>0.4709</td>\n",
       "      <td>0.4381</td>\n",
       "      <td>0.4774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseline_title</td>\n",
       "      <td>0.8093</td>\n",
       "      <td>0.8228</td>\n",
       "      <td>0.7910</td>\n",
       "      <td>0.4478</td>\n",
       "      <td>0.4136</td>\n",
       "      <td>0.4549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baseline_title</td>\n",
       "      <td>0.8093</td>\n",
       "      <td>0.8228</td>\n",
       "      <td>0.7910</td>\n",
       "      <td>0.4478</td>\n",
       "      <td>0.4136</td>\n",
       "      <td>0.4549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              exp_name  Train Acc  Valid Acc  Test  Acc  Train LogLoss  \\\n",
       "0             baseline     0.7947     0.7911     0.7687         0.4647   \n",
       "1  baseline_gridsearch     0.7960     0.7911     0.7687         0.4709   \n",
       "2       baseline_title     0.8093     0.8228     0.7910         0.4478   \n",
       "3       baseline_title     0.8093     0.8228     0.7910         0.4478   \n",
       "\n",
       "   Valid LogLoss  Test  LogLoss  \n",
       "0         0.4344         0.4783  \n",
       "1         0.4381         0.4774  \n",
       "2         0.4136         0.4549  \n",
       "3         0.4136         0.4549  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "exp_name = \"baseline_title\"\n",
    "try:\n",
    "    expLog\n",
    "except NameError:\n",
    "   expLog = pd.DataFrame(columns=[\"exp_name\", \n",
    "                                   \"Train Acc\", \n",
    "                                   \"Valid Acc\",\n",
    "                                   \"Test  Acc\",\n",
    "                                   \"Train LogLoss\", \n",
    "                                   \"Valid LogLoss\",\n",
    "                                   \"Test  LogLoss\"\n",
    "                                  ])\n",
    "\n",
    "expLog.loc[len(expLog)] = [f\"{exp_name}\"] + list(np.round(\n",
    "               [accuracy_score(y_train, model.predict(X_train)), \n",
    "                accuracy_score(y_valid, model.predict(X_valid)),\n",
    "                accuracy_score(y_test, model.predict(X_test)),\n",
    "                log_loss(y_train, model.predict_proba(X_train)),\n",
    "                log_loss(y_valid, model.predict_proba(X_valid)),\n",
    "                log_loss(y_test, model.predict_proba(X_test))],\n",
    "    4)) \n",
    "expLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title feature led to a reduction in validation error of : 17.889%\n",
      "Title feature led to a reduction in Test       error of : 10.67%\n",
      "Yay to the Title feature!!!\n"
     ]
    }
   ],
   "source": [
    "reduction_in_valid_error = 100*(expLog[\"Valid Acc\"][2] - expLog[\"Valid Acc\"][1]) /(1 - expLog[\"Valid Acc\"][2])\n",
    "reduction_in_test_error = 100*(expLog[\"Test  Acc\"][2] - expLog[\"Test  Acc\"][1]) /(1 - expLog[\"Test  Acc\"][2])\n",
    "print(f\"Title feature led to a reduction in validation error of : {np.round(reduction_in_valid_error, 3)}%\")\n",
    "print(f\"Title feature led to a reduction in Test       error of : {np.round(reduction_in_test_error, 3)}%\")\n",
    "print(\"Yay to the Title feature!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CaseStudy: 20newsgroups via a Column Transformer pipeline\n",
    "\n",
    "\n",
    "Datasets can often contain components of that require different feature\n",
    "extraction and processing pipelines.  This scenario might occur when:\n",
    "\n",
    "1. Your dataset consists of heterogeneous data types (e.g. raster images and\n",
    "   text captions)\n",
    "2. Your dataset is stored in a Pandas DataFrame and different columns\n",
    "   require different processing pipelines.\n",
    "\n",
    "This example demonstrates how to use\n",
    ":class:`sklearn.compose.ColumnTransformer` on a dataset containing\n",
    "different types of features.  We use the 20-newsgroups dataset and compute\n",
    "standard bag-of-words features for the subject line and body in separate\n",
    "pipelines as well as ad hoc features on the body. We combine them (with\n",
    "weights) using a ColumnTransformer and finally train a classifier on the\n",
    "combined set of features.\n",
    "\n",
    "The choice of features is not particularly helpful, but serves to illustrate\n",
    "the technique.\n",
    "\n",
    "This section is based on the following SciKit-Learn example:\n",
    "\n",
    "* [20newsgroups via a Column Transformer pipeline](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer.html#sphx-glr-auto-examples-compose-plot-column-transformer-py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets can often contain components of that require different feature extraction and processing pipelines. This scenario might occur when:\n",
    "\n",
    "Your dataset consists of heterogeneous data types (e.g. raster images and text captions)\n",
    "Your dataset is stored in a Pandas DataFrame and different columns require different processing pipelines.\n",
    "This example demonstrates how to use sklearn.compose.ColumnTransformer on a dataset containing different types of features. We use the 20-newsgroups dataset and compute standard bag-of-words features for the subject line and body in separate pipelines as well as ad hoc features on the body. We combine them (with weights) using a ColumnTransformer and finally train a classifier on the combined set of features.\n",
    "\n",
    "The choice of features is not particularly helpful, but serves to illustrate the technique.\n",
    "\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer.html#sphx-glr-auto-examples-compose-plot-column-transformer-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.82      0.73       259\n",
      "           1       0.81      0.66      0.73       311\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       570\n",
      "   macro avg       0.74      0.74      0.73       570\n",
      "weighted avg       0.75      0.73      0.73       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Author: Matt Terry <matt.terry@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': text.count('.')}\n",
    "                for text in posts]\n",
    "\n",
    "\n",
    "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
    "\n",
    "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
    "    `subject` and `body`.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        # construct object dtype array with two columns\n",
    "        # first column = 'subject' and second column = 'body'\n",
    "        features = np.empty(shape=(len(posts), 2), dtype=object)\n",
    "        for i, text in enumerate(posts):\n",
    "            headers, _, bod = text.partition('\\n\\n')\n",
    "            bod = strip_newsgroup_footer(bod)\n",
    "            bod = strip_newsgroup_quoting(bod)\n",
    "            features[i, 1] = bod\n",
    "\n",
    "            prefix = 'Subject:'\n",
    "            sub = ''\n",
    "            for line in headers.split('\\n'):\n",
    "                if line.startswith(prefix):\n",
    "                    sub = line[len(prefix):]\n",
    "                    break\n",
    "            features[i, 0] = sub\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Extract the subject & body\n",
    "    ('subjectbody', SubjectBodyExtractor()),\n",
    "\n",
    "    # Use ColumnTransformer to combine the features from subject and body\n",
    "    ('union', ColumnTransformer(\n",
    "        [\n",
    "            # Pulling features from the post's subject line (first column)\n",
    "            ('subject', TfidfVectorizer(min_df=50), 0),\n",
    "\n",
    "            # Pipeline for standard bag-of-words model for body (second column)\n",
    "            ('body_bow', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('best', TruncatedSVD(n_components=50)),\n",
    "            ]), 1),\n",
    "\n",
    "            # Pipeline for pulling ad hoc features from post's body\n",
    "            ('body_stats', Pipeline([\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ]), 1),\n",
    "        ],\n",
    "\n",
    "        # weight components in ColumnTransformer\n",
    "        transformer_weights={\n",
    "            'subject': 0.8,\n",
    "            'body_bow': 0.5,\n",
    "            'body_stats': 1.0,\n",
    "        }\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    # Linear Support Vector Classification\n",
    "    ('svc', LinearSVC()),\n",
    "])\n",
    "\n",
    "# limit the list of categories to make running this example faster.\n",
    "categories = ['alt.atheism', 'talk.religion.misc']\n",
    "train = fetch_20newsgroups(random_state=1,\n",
    "                           subset='train',\n",
    "                           categories=categories,\n",
    "                           )\n",
    "test = fetch_20newsgroups(random_state=1,\n",
    "                          subset='test',\n",
    "                          categories=categories,\n",
    "                          )\n",
    "\n",
    "pipeline.fit(train.data, train.target)\n",
    "y = pipeline.predict(test.data)\n",
    "print(classification_report(y, test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
