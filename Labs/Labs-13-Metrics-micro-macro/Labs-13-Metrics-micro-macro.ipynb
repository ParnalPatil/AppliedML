{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics: Micro and Macro approaches\n",
    "In this lab we are going to review the used metrics in a multiclass classification (precision, recall and F1 score using micro and macro approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "We will be using Precision, Recall and F1-Scores. For more information, please see:\n",
    "* [here](\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) \n",
    "* And  [here](http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics) for precision, recall, f-measure metrics.  \n",
    "\n",
    "### Terms\n",
    "\n",
    "* __True positives (TP):__ is an outcome where the model correctly predicts the positive class. \n",
    "* __True negatives (TN):__ is an outcome where the model correctly predicts the negative class.\n",
    "* __False positives (FP):__ is an outcome where the model incorrectly predicts the positive class\n",
    "* __False negatives (FN)__: is an outcome where the model incorrectly predicts the negative class.\n",
    "\n",
    "### Equation Precision\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.  \n",
    "$\n",
    "\\text{precision} = \\cfrac{TP}{TP + FP}\n",
    "$\n",
    "\n",
    "\n",
    "### Equation Recall\n",
    "Recall is the ratio of correctly predicted positive observations to the all observations in actual positive class.  \n",
    "$\n",
    "\\text{recall} = \\cfrac{TP}{TP + FN}\n",
    "$\n",
    "\n",
    "\n",
    "### Equation  $F_1$ score\n",
    "F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account.  \n",
    "$\n",
    "F_1 = \\cfrac{2}{\\cfrac{1}{\\text{precision}} + \\cfrac{1}{\\text{recall}}} = 2 \\times \\cfrac{\\text{precision}\\, \\times \\, \\text{recall}}{\\text{precision}\\, + \\, \\text{recall}} = \\cfrac{TP}{TP + \\cfrac{FN + FP}{2}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micro and macro approaches\n",
    "## Micro precision\n",
    "Micro averaging treats the entire set of data as an aggregate result, and calculates 1 metric rather than k metrics that get averaged together.\n",
    "\n",
    "For precision, this works by calculating all of the true positive results for each class and using that as the numerator, and then calculating all of the true positive and false positive results for each class, and using that as the denominator.  \n",
    "$\n",
    "\\text{precision micro }=\\cfrac {TP_1+TP_2+…+TP_k}{(TP_1+TP_2+…+TP_k)+(FP_1+FP_2+…+FP_k)}\n",
    "$   \n",
    "In this case, rather than each class having equal weight, each observation gets equal weight. This gives the classes with the most observations more power.\n",
    "## Macro precision\n",
    "Macro averaging reduces your multiclass predictions down to multiple sets of binary predictions, calculates the corresponding metric for each of the binary cases, and then averages the results together. As an example, consider precision for the binary case.\n",
    "\n",
    " $\n",
    "\\text{precision} = \\cfrac{TP}{TP + FP}\n",
    "$\n",
    "\n",
    "In the multiclass case, if there are  classes 1, 2 and 3 , macro averaging reduces the problem to multiple one-vs-all comparisons. The truth and estimate columns are recoded such that the only two classes are 1 and other, and then precision is calculated based on those recoded columns, with 1 being the “relevant” column. This process is repeated for the other 3 levels to get a total of 4 precision values. The results are then averaged together.\n",
    "\n",
    "The formula representation looks like this. For k classes:  \n",
    "$\n",
    "\\text{precision macro }=\\cfrac {Pr_1+Pr_2+…+Pr_k}{k}\n",
    "$  \n",
    "where $Pr_1$ is the precision calculated from recoding the multiclass predictions down to just class 1 and other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Multiclass classifier & micro and macro precision \n",
    "Suppose we have a multiclass classification problem with three classes where:\n",
    "\n",
    "* 98% of the points belong to Class 1\n",
    "* 1% of the points belong to Class 2\n",
    "* 1% of the points belong to Class 3 \n",
    "* We have a classifier that always predicts class 1.   \n",
    "What will the precision be based on micro approach? what about macro?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "#actual_labels: 98 elements class 1, 1 element class 2, 1 element class 3\n",
    "actual_labels= np.ones(98)\n",
    "actual_labels= np.append(actual_labels,[2,3])\n",
    "#the predictions ( all ones)\n",
    "predictions= np.ones(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating metrics using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score for each class is : [0.98 0.   0.  ]\n",
      "Precision score using micro approach : 0.98\n",
      "Precision score using macro approach : 0.327\n",
      "Recall score using micro approach : 0.98\n",
      "Recall score using macro approach : 0.333\n",
      "F1 score using micro approach : 0.98\n",
      "F1 score using macro approach : 0.33\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision score for each class is :\", precision_score(actual_labels, predictions, average=None))\n",
    "print(\"Precision score using micro approach :\", precision_score(actual_labels, predictions, average='micro'))\n",
    "print(\"Precision score using macro approach :\", np.round(precision_score(actual_labels, predictions, average='macro'),3))\n",
    "print(\"Recall score using micro approach :\", recall_score(actual_labels, predictions, average='micro'))\n",
    "print(\"Recall score using macro approach :\", np.round(recall_score(actual_labels, predictions, average='macro'),3))\n",
    "print(\"F1 score using micro approach :\", f1_score(actual_labels, predictions, average='micro'))\n",
    "print(\"F1 score using macro approach :\", np.round(f1_score(actual_labels, predictions, average='macro'),3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The micro approach is not very discriminating compared to the macro approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating macro precision manually (Decomposing into 3 binary classifiers) \n",
    "In this section we are trying to reproduce the macro presion results obtained by using sklearn. \n",
    "The macro precision reduces your multiclass predictions down to multiple sets of binary predictions, calculates the corresponding metric for each of the binary cases, and then averages the results together.\n",
    "In the multiclass case, if there are classes 1, 2 and 3 , macro averaging reduces the problem to multiple one-vs-all comparisons. The truth and estimate columns are recoded such that the only two classes are 1 and other, and then precision is calculated based on those recoded columns, with 1 being the “relevant” column. This process is repeated for the other 3 levels to get a total of 4 precision values. The results are then averaged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score for  the first binary classifier (class 1 vs class 2 and 3) : 0.98\n",
      "Precision score for  the second binary classifier (class 2 vs class 1 and 3): 0.01\n",
      "Precision score for  the third binary classifier (class 3 vs class 1 and 2): 0.01\n",
      "Precision score macro (average of the 3 binary classifiers precision) : 0.327\n",
      "Precision score macro using sklearn: 0.327\n"
     ]
    }
   ],
   "source": [
    "#Classifier prediction ( all ones)\n",
    "predictions= np.ones(100)\n",
    "# Labels of the first binary classifier (class 1 vs class 2 and 3)\n",
    "#98 ones and 2 zeroes\n",
    "labels1= np.ones(98) \n",
    "labels1= np.append(labels1,[0,0])\n",
    "# Labels of the second binary classifier (class 2 vs class 1 and 3)\n",
    "#99 zeroes and 1 one\n",
    "labels2= np.zeros(98) \n",
    "labels2= np.append(labels2,[1,0])\n",
    "# Labels of the third binary classifier (class 3 vs class 1 and 2)\n",
    "#99 zeroes and 1 one\n",
    "labels3= np.zeros(99) \n",
    "labels3= np.append(labels3,1)\n",
    "\n",
    "print(\"Precision score for  the first binary classifier (class 1 vs class 2 and 3) :\", precision_score(labels1, predictions, average=None)[1])\n",
    "print(\"Precision score for  the second binary classifier (class 2 vs class 1 and 3):\", precision_score(labels2, predictions, average=None)[1])\n",
    "print(\"Precision score for  the third binary classifier (class 3 vs class 1 and 2):\", precision_score(labels3, predictions, average=None)[1])\n",
    "print(\"Precision score macro (average of the 3 binary classifiers precision) :\", np.round(0.9802/3,3))\n",
    "print(\"Precision score macro using sklearn:\", np.round(precision_score(actual_labels, predictions, average='macro'),3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equal precision, recall and F1 score when using micro averaging with multi-class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    " $\n",
    "\\text{precision } P = \\cfrac{TP}{TP + FP}\n",
    "$  \n",
    "__Precision__ can be intuitively understood as the classifier’s ability to only predict really positive samples as positive. For example, a classifier that classifies just everything as positive would have a precision of 0.5 in a balanced test set (50% positive, 50% negative). One that has no false positives, i.e. classifies only the true positives as positive would have a precision of 1.0. So basically, the less false positives a classifier gives, the higher is its precision.  \n",
    "### Recall\n",
    " $\n",
    "\\text{recall } R= \\cfrac{TP}{TP + FN}\n",
    "$  \n",
    "__Recall__ can be interpreted as the amount of positive test samples that were actually classified as positive. A classifier that just outputs positive for every sample, regardless if it is really positive, would get a recall of 1.0 but a lower precision. The less false negatives a clasifier gives, the higher is its recall.\n",
    "\n",
    "So the higher precision and recall are, the better the classifier performs because it detects most of the positive samples (high recall) and does not detect many samples that should not be detected (high precision). In order to quantify that, we can use another metric called F1 score.\n",
    "### F1 score\n",
    " $\n",
    "\\text{F1 score } 𝐹_1=2 \\cfrac{𝑃∗𝑅}{𝑃+𝑅}\n",
    "$  \n",
    "This is just the weighted average between precision and recall. The higher precision and recall are, the higher the F1 score is. You can directly see from this formula, that if $𝑃=𝑅$ , then $𝐹_1=𝑃=𝑅$, because:  \n",
    "$\n",
    "𝐹_1=2 \\cfrac{𝑃∗𝑅}{𝑃+𝑅}=2\\cfrac{P∗P}{𝑃+P}=2\\cfrac{𝑃^2}{2𝑃}=𝑃\n",
    "$  \n",
    "So this already explains why the F1 score is the same as precision and recall, if precision and recall are the same. But why are recall and precision the same when using micro averaging? Let’s look at an example to understand this.\n",
    "\n",
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"example.png\" alt=\"example\" style=\"width: =600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TP is the amount of samples that were predicted to have the correct label.  \n",
    "* __TP = 4 (all green cells)__\n",
    "* FP is the amount of labels that got a “vote” but shouldn’t. For example, in the first column, 1 should have been predicted, but 2 was predicted. So there is a false positive for class 2 in this case. On the other hand, if the prediction is right (column 2), there is no FP counted. \n",
    "* __FP = 5 (all red cells)__\n",
    "* FN is the amount of labels that should have been predicted, but weren’t. Look at the first column again. 1 should have been predicted, but wasn’t. So there is a FN for class 1 in this case. As in the FP case, there is no FN counted if the prediction is correct (column 2).\n",
    "* __FP = 5 (all red cells) __\n",
    "\n",
    "* In other words, if there is a false positive, there will always also be a false negative and vice versa, because always one class if predicted. If class A is predicted and the true label is B, then there is a FP for A and a FN for B. If the prediction is correct, i.e. class A is predicted and A is also the true label,then there is neither a false positive nor a false negative but only a true positive. So there is no possibility that would increase only FP or FN but not both. That is why precision and recall are always the same when using the micro averaging scheme.\n",
    "\n",
    "Now let’s actually calculate the values of precision, recall and F1 score.\n",
    " * $\n",
    "\\text{ precision } P = \\cfrac{4}{4+5}= \\cfrac{4}{9}= 0.44 \n",
    "$   \n",
    " * $\n",
    "\\text{ recall  } R = \\cfrac{4}{4+5}= \\cfrac{4}{9}= 0.44 \n",
    "$   \n",
    " * $\n",
    "\\text{ F1 score  } 𝐹_1 = 2\\cfrac{4/9∗4/9}{4/9+4/9}= \\cfrac{4}{9}= 0.44 \n",
    "$\n",
    "\n",
    "We can see that all metric values are identical.\n",
    "\n",
    "Note: Since micro averaging does not distinguish between different classes and then just averages their metric scores, this averaging scheme is not prone to inaccurate values due to an unequally distributed test set (e.g. 3 classes and one of these has 98% of the samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (micro): 0.444444\n",
      "Recall (micro):    0.444444\n",
      "F1 score (micro):  0.444444\n",
      "\n",
      "Precision (macro): 0.366667\n",
      "Recall (macro):    0.361111\n",
      "F1 score (macro):  0.355556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# These values are the same as in the table above\n",
    "labels      = [1,2,3,2,3,3,1,2,2]\n",
    "predicitons = [2,2,1,2,1,3,2,3,2]\n",
    "print(\"Precision (micro): %f\" % precision_score(labels, predicitons, average='micro'))\n",
    "print(\"Recall (micro):    %f\" % recall_score(labels, predicitons, average='micro'))\n",
    "print(\"F1 score (micro):  %f\" % f1_score(labels, predicitons, average='micro'), end='\\n\\n')\n",
    "\n",
    "\n",
    "print(\"Precision (macro): %f\" % precision_score(labels, predicitons, average='macro'))\n",
    "print(\"Recall (macro):    %f\" % recall_score(labels, predicitons, average='macro'))\n",
    "print(\"F1 score (macro):  %f\" % f1_score(labels, predicitons, average='macro'), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study: Iris Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import  datasets\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding noisy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "originial X shape (150, 4)\n",
      "X shape after adding noisy features (150, 24)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"originial X shape\", X.shape)\n",
    "\n",
    "# Add noisy features\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 5*n_features)]\n",
    "\n",
    "print(\"X shape after adding noisy features\", X.shape)\n",
    "# Split into training, validation and test\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariemloukil/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Run classifier\n",
    "classifier = LogisticRegression(multi_class='multinomial',random_state=42,solver = 'lbfgs')\n",
    "model= classifier.fit(X_train, y_train)\n",
    "y_score_test= model.predict(X_test)\n",
    "y_score_valid = model.predict(X_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>Precision micro</th>\n",
       "      <th>Precision macro</th>\n",
       "      <th>recall micro</th>\n",
       "      <th>recall macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test set</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>validation set</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              set   Precision micro  Precision macro  recall micro  \\\n",
       "0        test set             0.900            0.905         0.900   \n",
       "1  validation set             0.913            0.907         0.913   \n",
       "\n",
       "   recall macro  \n",
       "0         0.905  \n",
       "1         0.907  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Precision and Recall\n",
    "\n",
    "results = pd.DataFrame(columns=[\"set \", \"Precision micro\", \"Precision macro\",\"recall micro\", \"recall macro\"])\n",
    "results.loc[len(results)] = [\"test set \", np.round(precision_score(y_test, y_score_test, average='micro'),3), \n",
    "                            np.round(precision_score(y_test, y_score_test,  average='macro'),3),np.round(recall_score(y_test, y_score_test, average='micro'),3),\n",
    "                            np.round(recall_score(y_test, y_score_test, average='macro'),3)]\n",
    "\n",
    "results.loc[len(results)] = [\"validation set \", np.round(precision_score(y_valid, y_score_valid, average='micro'),3), \n",
    "                            np.round(precision_score(y_valid, y_score_valid,  average='macro'),3),np.round(recall_score(y_valid, y_score_valid, average='micro'),3),\n",
    "                            np.round(recall_score(y_valid, y_score_valid, average='macro'),3)]\n",
    "\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
